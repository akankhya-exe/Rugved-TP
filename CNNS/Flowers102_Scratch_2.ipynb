{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmkAA2ktl8FQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import scipy.io\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "if os.path.exists('flowers_sorted'):\n",
        "    shutil.rmtree('flowers_sorted')\n",
        "\n",
        "labels_path = 'flowers/flowers-102/imagelabels.mat'\n",
        "source_dir = 'flowers/flowers-102/jpg'\n",
        "target_dir = 'flowers_sorted'\n",
        "\n",
        "labels = scipy.io.loadmat(labels_path)['labels'][0]\n",
        "\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "for i in range(len(labels)):\n",
        "    label = labels[i]\n",
        "    class_dir = os.path.join(target_dir, str(label))\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "    image_filename = f'image_{i+1:05d}.jpg'\n",
        "    source_path = os.path.join(source_dir, image_filename)\n",
        "    target_path = os.path.join(class_dir, image_filename)\n",
        "    if os.path.exists(source_path):\n",
        "        shutil.copy(source_path, target_path)\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "full_dataset = ImageFolder(root='flowers_sorted', transform=data_transforms['train'])\n",
        "full_dataset_val = ImageFolder(root='flowers_sorted', transform=data_transforms['val'])\n",
        "\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "val_size = int(0.1 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "print(f\"\\nTotal images: {total_size}\")\n",
        "print(f\"Splitting into: Train: {train_size}, Validation: {val_size}, Test: {test_size}\")\n",
        "\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_indices, val_indices, test_indices = random_split(range(total_size), [train_size, val_size, test_size], generator=generator)\n",
        "\n",
        "train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
        "val_dataset = torch.utils.data.Subset(full_dataset_val, val_indices)\n",
        "test_dataset = torch.utils.data.Subset(full_dataset_val, test_indices)\n",
        "\n",
        "batch_size = 32\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
        "    'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=False),\n",
        "    'test': DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "}\n",
        "dataset_sizes = { 'train': len(train_dataset), 'val': len(val_dataset), 'test': len(test_dataset) }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUOGl5dkzo0s",
        "outputId": "7970186f-57cb-4ff3-b95a-9e119193c646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing old 'flowers_sorted' directory...\n",
            "Organizing files...\n",
            "File organization complete!\n",
            "\n",
            "Total images: 8189\n",
            "Splitting into -> Train: 6551, Validation: 818, Test: 820\n",
            "\n",
            "DataLoaders created successfully with the new 80/10/10 split!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=102):\n",
        "        super(DeeperCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 14 * 14, 1024), nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN(num_classes=102).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "m37cbr03pgmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7dfd40d-512a-45bb-996a-059db08fa454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model, Loss, and Optimizer are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs=25, patience=5):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_val_loss = np.inf\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}' + '\\n' + '-'*10)\n",
        "        for phase in ['train', 'val']:\n",
        "            model.train() if phase == 'train' else model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'val':\n",
        "                if epoch_loss < best_val_loss:\n",
        "                    best_val_loss = epoch_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {patience} epochs.\")\n",
        "            break\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Loss: {best_val_loss:4f}')\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, num_epochs=25, patience=5)"
      ],
      "metadata": {
        "id": "mNOyQZmYsGU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e777c6-7b65-4f32-a9c1-08d530aa53b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss: 4.0952 Acc: 0.0696\n",
            "val Loss: 3.5977 Acc: 0.1125\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss: 3.5242 Acc: 0.1330\n",
            "val Loss: 3.1693 Acc: 0.1993\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss: 3.2269 Acc: 0.1902\n",
            "val Loss: 2.8499 Acc: 0.2738\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss: 2.9928 Acc: 0.2354\n",
            "val Loss: 2.5810 Acc: 0.3411\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss: 2.8040 Acc: 0.2754\n",
            "val Loss: 2.3808 Acc: 0.3704\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss: 2.6273 Acc: 0.3132\n",
            "val Loss: 2.1552 Acc: 0.4267\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss: 2.4710 Acc: 0.3457\n",
            "val Loss: 2.0767 Acc: 0.4352\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss: 2.3747 Acc: 0.3770\n",
            "val Loss: 2.0125 Acc: 0.4939\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss: 2.2572 Acc: 0.4007\n",
            "val Loss: 1.8482 Acc: 0.5024\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss: 2.1556 Acc: 0.4328\n",
            "val Loss: 1.7681 Acc: 0.5330\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss: 2.1130 Acc: 0.4296\n",
            "val Loss: 1.6508 Acc: 0.5672\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss: 2.0018 Acc: 0.4631\n",
            "val Loss: 1.6377 Acc: 0.5660\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss: 1.9780 Acc: 0.4630\n",
            "val Loss: 1.5809 Acc: 0.5868\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss: 1.9073 Acc: 0.4822\n",
            "val Loss: 1.5175 Acc: 0.5905\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss: 1.8368 Acc: 0.5033\n",
            "val Loss: 1.4877 Acc: 0.6027\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss: 1.7804 Acc: 0.5198\n",
            "val Loss: 1.4408 Acc: 0.6064\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss: 1.7430 Acc: 0.5288\n",
            "val Loss: 1.4350 Acc: 0.6100\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss: 1.6996 Acc: 0.5390\n",
            "val Loss: 1.3563 Acc: 0.6308\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss: 1.6687 Acc: 0.5425\n",
            "val Loss: 1.3154 Acc: 0.6491\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss: 1.6216 Acc: 0.5575\n",
            "val Loss: 1.3974 Acc: 0.6345\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss: 1.6146 Acc: 0.5633\n",
            "val Loss: 1.2472 Acc: 0.6675\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss: 1.5590 Acc: 0.5733\n",
            "val Loss: 1.2266 Acc: 0.6785\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss: 1.5340 Acc: 0.5805\n",
            "val Loss: 1.2481 Acc: 0.6614\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss: 1.5017 Acc: 0.5869\n",
            "val Loss: 1.2329 Acc: 0.6785\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss: 1.4894 Acc: 0.5946\n",
            "val Loss: 1.2276 Acc: 0.6834\n",
            "\n",
            "Training complete in 21m 54s\n",
            "Best val Loss: 1.226625\n"
          ]
        }
      ]
    }
  ]
}
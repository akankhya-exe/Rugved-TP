{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av8sbaTQpkeF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuJZAib658GJ"
      },
      "outputs": [],
      "source": [
        "def zero_pad(X, pad):\n",
        "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode='constant', constant_values=0)\n",
        "\n",
        "    return X_pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNVOzISLUsB3"
      },
      "source": [
        "zero_pad: pads the image zeros or any value specified as pad (3, 5, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9mC6Nwd597a"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "np.random.seed(1)\n",
        "x = np.random.randn(4, 3, 3, 2)\n",
        "x_pad = zero_pad(x, 2)\n",
        "\n",
        "print (\"x.shape =\", x.shape)\n",
        "print (\"x_pad.shape =\", x_pad.shape)\n",
        "\n",
        "# Show a padded image\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_pad[0, :, :, 0])\n",
        "plt.title(\"Padded image example\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGDmrQp_6FwH"
      },
      "outputs": [],
      "source": [
        "def conv_single_step(a_slice_prev, W, b):\n",
        "    s = a_slice_prev * W\n",
        "    Z = np.sum(s)\n",
        "    Z = Z + float(b)\n",
        "\n",
        "    return Z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6etlnAiWASW"
      },
      "source": [
        "a_slice_prev: small patch of the input image with full depth\n",
        "W: weights of each cell of the filter grid\n",
        "b: bias\n",
        "Places the filter on the patch a_slice_prev (multiplication), adds all values and puts them in Z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgf8hsG56VqI"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "np.random.seed(1)\n",
        "a_slice_prev = np.random.randn(4, 4, 3)\n",
        "W = np.random.randn(4, 4, 3)\n",
        "b = np.random.randn(1, 1, 1)\n",
        "\n",
        "Z = conv_single_step(a_slice_prev, W, b)\n",
        "print(\"Z =\", Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "livbGpzy6Xl0"
      },
      "outputs": [],
      "source": [
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    # Forward prop\n",
        "\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "\n",
        "    stride = hparameters['stride']\n",
        "    pad = hparameters['pad']\n",
        "\n",
        "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
        "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
        "\n",
        "    Z = np.zeros((m, n_H, n_W, n_C))\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "\n",
        "    for i in range(m):\n",
        "        a_prev_pad = A_prev_pad[i]\n",
        "        for h in range(n_H):\n",
        "            vert_start = h * stride\n",
        "            vert_end = vert_start + f\n",
        "\n",
        "            for w in range(n_W):\n",
        "                horiz_start = w * stride\n",
        "                horiz_end = horiz_start + f\n",
        "\n",
        "                for c in range(n_C):\n",
        "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "                    weights = W[:, :, :, c]\n",
        "                    biases = b[:, :, :, c]\n",
        "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
        "\n",
        "\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "\n",
        "    return Z, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAqs09xXXAzt"
      },
      "source": [
        "Firstly, gets dimensions of the input A_prev and filter W, and hyperparameters like pad and stride, uses the formula to calculate output shape\n",
        "Then adds the padding around A_prev and stores the results in Z\n",
        "\n",
        "for i in range(m): -> For every trianing example\n",
        "for h in range(n_H): -> Moves row by row\n",
        "for w in range(n_W): -> Moves column by column\n",
        "for c in range(n_C): -> All filters are applied\n",
        "\n",
        "Find the patch, perform the convolution, store the result, cache it for backprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ6qZYhJ6jfM"
      },
      "outputs": [],
      "source": [
        "# Testing\n",
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(10, 4, 4, 3)\n",
        "W = np.random.randn(2, 2, 3, 8)\n",
        "b = np.random.randn(1, 1, 1, 8)\n",
        "hparameters = {\"pad\" : 2, \"stride\": 2}\n",
        "\n",
        "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
        "print(\"Z's mean =\", np.mean(Z))\n",
        "print(\"Z[3,2,1] =\", Z[3, 2, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLpwAN056lkh"
      },
      "outputs": [],
      "source": [
        "def pool_forward(A_prev, hparameters, mode=\"max\"):\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "\n",
        "    f = hparameters[\"f\"]\n",
        "    stride = hparameters[\"stride\"]\n",
        "\n",
        "    n_H = int(1 + (n_H_prev - f) / stride)\n",
        "    n_W = int(1 + (n_W_prev - f) / stride)\n",
        "    n_C = n_C_prev\n",
        "\n",
        "    A = np.zeros((m, n_H, n_W, n_C))\n",
        "\n",
        "    for i in range(m):\n",
        "        for h in range(n_H):\n",
        "            vert_start = h * stride\n",
        "            vert_end = vert_start + f\n",
        "            for w in range(n_W):\n",
        "                horiz_start = w * stride\n",
        "                horiz_end = horiz_start + f\n",
        "                for c in range(n_C):\n",
        "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "                    if mode == \"max\":\n",
        "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
        "                    elif mode == \"average\":\n",
        "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
        "\n",
        "    cache = (A_prev, hparameters)\n",
        "\n",
        "    assert(A.shape == (m, n_H, n_W, n_C))\n",
        "\n",
        "    return A, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-aOoITwZMRj"
      },
      "source": [
        "Same as the last function, pass through small patches and store the average or max value of that patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrS_5yq36uLj"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(2, 4, 4, 3)\n",
        "hparameters = {\"stride\" : 2, \"f\": 3}\n",
        "\n",
        "A, cache = pool_forward(A_prev, hparameters)\n",
        "print(\"mode = max\")\n",
        "print(\"A =\", A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPCV_dZ168XR"
      },
      "outputs": [],
      "source": [
        "def create_mask_from_window(x):\n",
        "    mask = (x == np.max(x))\n",
        "    return mask\n",
        "\n",
        "def distribute_value(dz, shape):\n",
        "    (n_H, n_W) = shape\n",
        "    average = dz / (n_H * n_W)\n",
        "    a = np.ones(shape) * average\n",
        "    return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWL8XnAA69NR"
      },
      "outputs": [],
      "source": [
        "def conv_backward(dZ, cache):\n",
        "    (A_prev, W, b, hparameters) = cache\n",
        "\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "\n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "\n",
        "    (m, n_H, n_W, n_C) = dZ.shape\n",
        "\n",
        "    dA_prev = np.zeros(A_prev.shape)\n",
        "    dW = np.zeros(W.shape)\n",
        "    db = np.zeros(b.shape)\n",
        "\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
        "\n",
        "    for i in range(m):\n",
        "        a_prev_pad = A_prev_pad[i]\n",
        "        da_prev_pad = dA_prev_pad[i]\n",
        "\n",
        "        for h in range(n_H):\n",
        "            for w in range(n_W):\n",
        "                for c in range(n_C):\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = horiz_start + f\n",
        "\n",
        "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
        "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
        "\n",
        "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
        "\n",
        "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sON7Bo1qOLb2"
      },
      "outputs": [],
      "source": [
        "def pool_backward(dA, cache, mode = \"max\"):\n",
        "    (A_prev, hparameters) = cache\n",
        "\n",
        "    stride = hparameters[\"stride\"]\n",
        "    f = hparameters[\"f\"]\n",
        "\n",
        "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "    m, n_H, n_W, n_C = dA.shape\n",
        "\n",
        "    dA_prev = np.zeros(A_prev.shape)\n",
        "\n",
        "    for i in range(m):\n",
        "        a_prev = A_prev[i]\n",
        "        for h in range(n_H):\n",
        "            for w in range(n_W):\n",
        "                for c in range(n_C):\n",
        "                    vert_start = h * stride\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end = horiz_start + f\n",
        "\n",
        "                    if mode == \"max\":\n",
        "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "                        mask = create_mask_from_window(a_prev_slice)\n",
        "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
        "\n",
        "                    elif mode == \"average\":\n",
        "                        da = dA[i, h, w, c]\n",
        "                        shape = (f, f)\n",
        "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
        "\n",
        "    assert(dA_prev.shape == A_prev.shape)\n",
        "\n",
        "    return dA_prev\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeZJb3di7wRs"
      },
      "outputs": [],
      "source": [
        "def relu_forward(Z):\n",
        "    A = np.maximum(0, Z)\n",
        "    cache = Z # Cache Z for backpropagation\n",
        "    return A, cache\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True)\n",
        "\n",
        "    dZ[Z <= 0] = 0\n",
        "\n",
        "    return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tfSgDx47xB4"
      },
      "outputs": [],
      "source": [
        "# Forward pass here needs flattened input (in the main model)\n",
        "\n",
        "def fc_forward(A_prev_flat, W, b):\n",
        "    Z = np.dot(W, A_prev_flat) + b\n",
        "    cache = (A_prev_flat, W, b)\n",
        "    return Z, cache\n",
        "\n",
        "def fc_backward(dZ, cache):\n",
        "    A_prev_flat, W, b = cache\n",
        "    m = A_prev_flat.shape[1]\n",
        "\n",
        "    dW = (1/m) * np.dot(dZ, A_prev_flat.T)\n",
        "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev_flat = np.dot(W.T, dZ)\n",
        "\n",
        "    return dA_prev_flat, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArLE2yK47zKK"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters_flowers(input_height, input_width, num_classes):\n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Convolutional layer (8 filters of size 4 x 4, input has 3 colour channels)\n",
        "    W1 = np.random.randn(4, 4, 3, 8) * 0.01\n",
        "    b1 = np.zeros((1, 1, 1, 8))\n",
        "\n",
        "    h_after_conv = int((input_height - 4 + 2*1) / 1) + 1\n",
        "    w_after_conv = int((input_width - 4 + 2*1) / 1) + 1\n",
        "\n",
        "    h_after_pool = int((h_after_conv - 2) / 2) + 1\n",
        "    w_after_pool = int((w_after_conv - 2) / 2) + 1\n",
        "\n",
        "    flattened_size = h_after_pool * w_after_pool * 8\n",
        "\n",
        "\n",
        "    W2 = np.random.randn(num_classes, flattened_size) * 0.01\n",
        "    b2 = np.zeros((num_classes, 1))\n",
        "\n",
        "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
        "\n",
        "    return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0q0jptI70sv"
      },
      "outputs": [],
      "source": [
        "def compute_cost(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    # To avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    cost = - (1/m) * np.sum(Y * np.log(AL + epsilon))\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost\n",
        "\n",
        "def model_forward(X, parameters, hparameters_conv, hparameters_pool):\n",
        "    # CONV -> RELU -> POOL -> FLATTEN -> FC\n",
        "    caches = []\n",
        "\n",
        "    W1, b1 = parameters['W1'], parameters['b1']\n",
        "    W2, b2 = parameters['W2'], parameters['b2']\n",
        "\n",
        "    # CONV layer\n",
        "    Z1, conv_cache = conv_forward(X, W1, b1, hparameters_conv)\n",
        "    caches.append(conv_cache)\n",
        "\n",
        "    # RELU activation\n",
        "    A1, relu_cache = relu_forward(Z1)\n",
        "    caches.append(relu_cache)\n",
        "\n",
        "    # POOL layer\n",
        "    A2, pool_cache = pool_forward(A1, hparameters_pool, mode=\"max\")\n",
        "    caches.append(pool_cache)\n",
        "\n",
        "    # FLATTEN\n",
        "    # Store original shape to unflatten in backprop\n",
        "    (m, n_H, n_W, n_C) = A2.shape\n",
        "    flatten_cache = A2.shape\n",
        "    caches.append(flatten_cache)\n",
        "    A2_flat = A2.reshape(m, -1).T\n",
        "\n",
        "    # FULLY CONNECTED layer\n",
        "    Z3, fc_cache = fc_forward(A2_flat, W2, b2)\n",
        "    caches.append(fc_cache)\n",
        "\n",
        "    # SOFTMAX final layer\n",
        "    AL = np.exp(Z3) / np.sum(np.exp(Z3), axis=0)\n",
        "\n",
        "    return AL, caches\n",
        "\n",
        "\n",
        "def model_backward(AL, Y, caches):\n",
        "    grads = {}\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape)\n",
        "\n",
        "    (conv_cache, relu_cache, pool_cache, flatten_cache, fc_cache) = caches\n",
        "\n",
        "    dZ3 = AL - Y\n",
        "\n",
        "    # FULLY CONNECTED backward\n",
        "    A2_flat, W2, b2 = fc_cache\n",
        "    dA2_flat, dW2, db2 = fc_backward(dZ3, fc_cache)\n",
        "    grads[\"dW2\"] = dW2\n",
        "    grads[\"db2\"] = db2\n",
        "\n",
        "    # UNFLATTEN\n",
        "    original_shape = flatten_cache\n",
        "    dA2 = dA2_flat.T.reshape(original_shape)\n",
        "\n",
        "    # POOL backward\n",
        "    dA1 = pool_backward(dA2, pool_cache, mode=\"max\")\n",
        "\n",
        "    # RELU backward\n",
        "    dZ1 = relu_backward(dA1, relu_cache)\n",
        "\n",
        "    # CONV backward\n",
        "    dA0, dW1, db1 = conv_backward(dZ1, conv_cache)\n",
        "    grads[\"dW1\"] = dW1\n",
        "    grads[\"db1\"] = db1\n",
        "\n",
        "    return grads\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    parameters[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
        "    parameters[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
        "    parameters[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
        "    parameters[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaRttrFv72Si"
      },
      "outputs": [],
      "source": [
        "def train_model(X, Y, input_shape, num_classes, learning_rate=0.01, num_epochs=100):\n",
        "    input_height, input_width, _ = input_shape\n",
        "    parameters = initialize_parameters_flowers(input_height, input_width, num_classes)\n",
        "    hparameters_conv = {\"pad\": 1, \"stride\": 1}\n",
        "    hparameters_pool = {\"stride\": 2, \"f\": 2}\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        AL, caches = model_forward(X, parameters, hparameters_conv, hparameters_pool)\n",
        "        cost = compute_cost(AL, Y)\n",
        "        grads = model_backward(AL, Y, caches)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        if i % 5 == 0 or i == num_epochs - 1:\n",
        "            costs.append(cost)\n",
        "            print(f\"Cost after epoch {i}: {cost:.4f}\")\n",
        "\n",
        "    return parameters, costs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAjg8uRp8bOe"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "def load_and_preprocess(image_size=(64, 64), max_examples=500):\n",
        "    (ds_train, ds_validation), ds_info = tfds.load(\n",
        "        'oxford_flowers102',\n",
        "        split=['train', 'validation'],\n",
        "        shuffle_files=True,\n",
        "        with_info=True,\n",
        "        as_supervised=True,\n",
        "    )\n",
        "\n",
        "    num_classes = ds_info.features['label'].num_classes\n",
        "\n",
        "    def preprocess(image, label):\n",
        "        image = tf.image.resize(image, image_size)\n",
        "        image = image / 255.0\n",
        "        label = tf.one_hot(label, depth=num_classes)\n",
        "        return image, label\n",
        "\n",
        "    ds_train = ds_train.map(preprocess).take(max_examples).cache().batch(max_examples)\n",
        "    ds_validation = ds_validation.map(preprocess).take(100).cache().batch(100)\n",
        "\n",
        "    for images, labels in ds_train:\n",
        "        X_train = images.numpy()\n",
        "        Y_train = labels.numpy().T\n",
        "\n",
        "    for images, labels in ds_validation:\n",
        "        X_val = images.numpy()\n",
        "        Y_val = labels.numpy().T\n",
        "\n",
        "    print(f\"Data loaded successfully.\")\n",
        "    print(f\"X_train shape: {X_train.shape}\")\n",
        "    print(f\"Y_train shape: {Y_train.shape}\")\n",
        "\n",
        "    return X_train, Y_train, X_val, Y_val, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tDyg-yXa8xIN"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (64, 64)\n",
        "MAX_EXAMPLES_TO_USE = 500 # Starting small\n",
        "\n",
        "X_train, Y_train, X_val, Y_val, NUM_CLASSES = load_and_preprocess(\n",
        "    image_size=IMAGE_SIZE,\n",
        "    max_examples=MAX_EXAMPLES_TO_USE\n",
        ")\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "\n",
        "trained_parameters, costs_history = train_model(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    input_shape,\n",
        "    NUM_CLASSES,\n",
        "    learning_rate=0.01,\n",
        "    num_epochs=50 # Try changing, 50 is really slow (tried 40 and 30)\n",
        ")\n",
        "\n",
        "print(\"Training finished\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}